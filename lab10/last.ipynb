{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0535a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "# Cell M1 - Imports and file paths\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Paths (change if needed)\n",
    "mnist_train_path = 'train.csv'\n",
    "mnist_test_path  = 'test.csv'\n",
    "\n",
    "# Quick check\n",
    "for p in (mnist_train_path, mnist_test_path):\n",
    "    if not os.path.exists(p):\n",
    "        print(f\"Warning: {p} not found. Please upload the file or change the path.\")\n",
    "        \n",
    "print(\"Imports ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "899a5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell M2 - Evaluation metrics implemented from scratch\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    cm = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[class_to_idx[t], class_to_idx[p]] += 1\n",
    "    return cm, classes\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def precision_recall_f1(y_true, y_pred, average='macro'):\n",
    "    # returns (precision, recall, f1)\n",
    "    cm, classes = confusion_matrix(y_true, y_pred)\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    for i in range(len(classes)):\n",
    "        tp = cm[i,i]\n",
    "        fp = cm[:,i].sum() - tp\n",
    "        fn = cm[i,:].sum() - tp\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1v = 2*prec*rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        precisions.append(prec); recalls.append(rec); f1s.append(f1v)\n",
    "    if average == 'macro':\n",
    "        return np.mean(precisions), np.mean(recalls), np.mean(f1s)\n",
    "    elif average == 'micro':\n",
    "        tp = sum(cm[i,i] for i in range(len(classes)))\n",
    "        fp = (cm.sum(axis=0) - np.diag(cm)).sum()\n",
    "        fn = (cm.sum(axis=1) - np.diag(cm)).sum()\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1v = 2*prec*rec / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        return prec, rec, f1v\n",
    "    else:\n",
    "        raise ValueError(\"average must be 'macro' or 'micro'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53b948b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (42000, 785)\n",
      "Test shape: (28000, 784)\n",
      "X_train shape: (42000, 784) y_train shape: (42000,)\n",
      "X_test shape: (28000, 784)  -- test labels not provided\n"
     ]
    }
   ],
   "source": [
    "# Cell M3 - Load MNIST train/test from CSV (format: label,pixel0,...,pixel783)\n",
    "if not os.path.exists(mnist_train_path) or not os.path.exists(mnist_test_path):\n",
    "    raise FileNotFoundError(\"MNIST CSV files not found at provided paths. Update paths and retry.\")\n",
    "\n",
    "mnist_train = pd.read_csv(mnist_train_path)\n",
    "mnist_test  = pd.read_csv(mnist_test_path)\n",
    "\n",
    "print(\"Train shape:\", mnist_train.shape)\n",
    "print(\"Test shape:\", mnist_test.shape)\n",
    "\n",
    "# Separate X,y\n",
    "y_train = mnist_train.iloc[:,0].values\n",
    "X_train = mnist_train.iloc[:,1:].values.astype(np.float32) / 255.0\n",
    "\n",
    "# If test.csv contains labels, use them; otherwise we only predict\n",
    "if mnist_test.shape[1] == 785:\n",
    "    y_test = mnist_test.iloc[:,0].values\n",
    "    X_test = mnist_test.iloc[:,1:].values.astype(np.float32) / 255.0\n",
    "else:\n",
    "    y_test = None\n",
    "    X_test = mnist_test.values.astype(np.float32) / 255.0\n",
    "    \n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
    "if y_test is not None:\n",
    "    print(\"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)\n",
    "else:\n",
    "    print(\"X_test shape:\", X_test.shape, \" -- test labels not provided\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d7b2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell M4 - Multiclass logistic regression using one-vs-rest (from scratch)\n",
    "\n",
    "class LogisticOVR:\n",
    "    def __init__(self, lr=0.5, n_iter=1000, fit_intercept=True, reg_lambda=0.0, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        if not self.fit_intercept:\n",
    "            return X\n",
    "        return np.hstack([np.ones((X.shape[0],1), dtype=X.dtype), X])\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.W = np.zeros((len(self.classes_), n_features + (1 if self.fit_intercept else 0)))\n",
    "        for idx, cls in enumerate(self.classes_):\n",
    "            # binary labels for this class\n",
    "            y_bin = (y == cls).astype(np.float32)\n",
    "            w = np.zeros(n_features + (1 if self.fit_intercept else 0), dtype=np.float32)\n",
    "            Xb = self._add_intercept(X)\n",
    "            for i in range(self.n_iter):\n",
    "                z = Xb.dot(w)\n",
    "                p = self._sigmoid(z)\n",
    "                error = p - y_bin\n",
    "                grad = (Xb.T @ error) / n_samples\n",
    "                # L2 regularization (do not regularize intercept)\n",
    "                if self.reg_lambda > 0:\n",
    "                    reg = np.concatenate(([0.0], self.reg_lambda * w[1:] / n_samples)) if self.fit_intercept else self.reg_lambda * w / n_samples\n",
    "                    grad += reg\n",
    "                w -= self.lr * grad\n",
    "                if self.verbose and (i % max(1, self.n_iter//5) == 0):\n",
    "                    loss = -np.mean(y_bin*np.log(p+1e-12) + (1-y_bin)*np.log(1-p+1e-12))\n",
    "                    print(f\"Class {cls} iter {i}/{self.n_iter} loss {loss:.4f}\")\n",
    "            self.W[idx] = w\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        Xb = self._add_intercept(X)\n",
    "        logits = Xb @ self.W.T  # shape (n_samples, n_classes)\n",
    "        probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "        # for OVR, normalize to pseudo-probabilities across classes\n",
    "        # but better to use softmax-like normalization: convert OVR-scores to probabilities by normalizing logits via softmax\n",
    "        # compute softmax of logits for multi-class probabilities\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probs_multi = exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "        return probs_multi  # shape (n_samples, n_classes)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return self.classes_[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c6454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic OVR (may take several minutes depending on dataset size and n_iter)...\n",
      "Training completed.\n",
      "Test labels not provided; predictions available in y_pred_test array.\n"
     ]
    }
   ],
   "source": [
    "# Cell M5 - Train OVR logistic on MNIST (can be slow; tune n_iter if needed)\n",
    "# Choose hyperparameters (these are moderate; adjust for speed)\n",
    "lr = 0.5\n",
    "n_iter = 800  # increase for better convergence, but slower\n",
    "reg_lambda = 0.01\n",
    "\n",
    "logreg = LogisticOVR(lr=lr, n_iter=n_iter, reg_lambda=reg_lambda, verbose=False)\n",
    "print(\"Training logistic OVR (may take several minutes depending on dataset size and n_iter)...\")\n",
    "logreg.fit(X_train, y_train)\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Predict\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "if y_test is not None:\n",
    "    acc = accuracy(y_test, y_pred_test)\n",
    "    prec, rec, f1 = precision_recall_f1(y_test, y_pred_test, average='macro')\n",
    "    print(f\"MNIST Test metrics (macro): Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}\")\n",
    "    cm, classes = confusion_matrix(y_test, y_pred_test)\n",
    "    print(\"Confusion matrix shape:\", cm.shape)\n",
    "else:\n",
    "    print(\"Test labels not provided; predictions available in y_pred_test array.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "999ea392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell M6 - Convert array->image and single-sample predict wrapper\n",
    "\n",
    "def array_to_image(arr, size=(28,28), scale=255, cmap='gray'):\n",
    "    \"\"\"\n",
    "    Convert 1D array length 784 (values in [0,1] or [0,255]) to PIL Image.\n",
    "    Returns PIL.Image.\n",
    "    \"\"\"\n",
    "    a = np.array(arr, dtype=np.float32).copy()\n",
    "    if a.max() <= 1.0:\n",
    "        a = a * scale\n",
    "    a = a.reshape(size)\n",
    "    # Convert to uint8 and PIL image\n",
    "    a_uint8 = np.clip(a, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(a_uint8, mode='L')\n",
    "\n",
    "def predict_single_image(arr784, model):\n",
    "    \"\"\"\n",
    "    arr784: 1D array length 784 (pixel intensities 0-255 or 0-1)\n",
    "    model: trained LogisticOVR instance\n",
    "    Returns: PIL image, predicted label\n",
    "    \"\"\"\n",
    "    x = np.array(arr784, dtype=np.float32).reshape(1, -1)\n",
    "    # If pixels appear in 0-255 range, scale to [0,1] as model trained on scaled\n",
    "    if x.max() > 1.0:\n",
    "        x = x / 255.0\n",
    "    pred = model.predict(x)[0]\n",
    "    img = array_to_image(x.ravel(), size=(28,28))\n",
    "    return img, int(pred)\n",
    "\n",
    "# Example usage if you want to test on the first test sample:\n",
    "# img, label_pred = predict_single_image(X_test[0]*255.0, logreg)  # pass original scale optionally\n",
    "# display(img); print(\"Predicted:\", label_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8b1999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bank dataset shape: (45211, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell B1 - Load bank-full.csv and set split parameters (reuse same split as Assignment 9: 80:20 with random_state=42)\n",
    "bank_path = 'bank-full.csv'\n",
    "if not os.path.exists(bank_path):\n",
    "    raise FileNotFoundError(\"bank-full.csv not found at /mnt/data. Upload or change path.\")\n",
    "\n",
    "bank_df = pd.read_csv(bank_path, sep=';')\n",
    "print(\"Bank dataset shape:\", bank_df.shape)\n",
    "bank_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01e35ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical cols: ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
      "Numeric cols: ['age', 'balance', 'day', 'campaign', 'pdays', 'previous']\n"
     ]
    }
   ],
   "source": [
    "# Cell B2 - Preprocessing for Naive Bayes\n",
    "\n",
    "def preprocess_bank(df, drop_duration=True):\n",
    "    df_proc = df.copy()\n",
    "    if drop_duration and 'duration' in df_proc.columns:\n",
    "        df_proc = df_proc.drop(columns=['duration'])\n",
    "    # Map target\n",
    "    df_proc['y'] = df_proc['y'].map({'no':0, 'yes':1})\n",
    "    # Identify categorical and numeric\n",
    "    cat_cols = df_proc.select_dtypes(include=['object']).columns.tolist()\n",
    "    num_cols = df_proc.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    num_cols.remove('y')\n",
    "    # Fill 'unknown' in categorical with a string 'unknown' (keep it as a category)\n",
    "    # For safety, replace missing with mode/median:\n",
    "    for c in cat_cols:\n",
    "        df_proc[c] = df_proc[c].fillna('unknown')\n",
    "    for n in num_cols:\n",
    "        df_proc[n] = df_proc[n].fillna(df_proc[n].median())\n",
    "    return df_proc, cat_cols, num_cols\n",
    "\n",
    "bank_proc, cat_cols, num_cols = preprocess_bank(bank_df, drop_duration=True)\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "print(\"Numeric cols:\", num_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2f97dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (36168, 16) Test shape: (9043, 16)\n"
     ]
    }
   ],
   "source": [
    "# Cell B3 - create 80:20 split (same as earlier assignment)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(bank_proc, test_size=0.2, shuffle=True, random_state=42)\n",
    "print(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)\n",
    "\n",
    "X_train_bank = train_df.drop(columns=['y']).reset_index(drop=True)\n",
    "y_train_bank = train_df['y'].values\n",
    "X_test_bank  = test_df.drop(columns=['y']).reset_index(drop=True)\n",
    "y_test_bank  = test_df['y'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aefe26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell B4 - Hybrid Naive Bayes classifier (from scratch)\n",
    "\n",
    "class HybridNaiveBayes:\n",
    "    def __init__(self, categorical_features, numeric_features, laplace=1.0):\n",
    "        self.cat_features = list(categorical_features)\n",
    "        self.num_features = list(numeric_features)\n",
    "        self.laplace = laplace\n",
    "        self.class_priors = {}\n",
    "        self.cat_cond_prob = {}  # dict[class][feature][value] = prob\n",
    "        self.num_params = {}     # dict[class][feature] = (mean, var)\n",
    "        self.classes_ = None\n",
    "    \n",
    "    def fit(self, X_df, y):\n",
    "        self.classes_, counts = np.unique(y, return_counts=True)\n",
    "        n = len(y)\n",
    "        # class priors\n",
    "        for cls, cnt in zip(self.classes_, counts):\n",
    "            self.class_priors[cls] = cnt / n\n",
    "        # categorical conditionals\n",
    "        for cls in self.classes_:\n",
    "            Xc = X_df[y == cls]\n",
    "            self.cat_cond_prob[cls] = {}\n",
    "            for f in self.cat_features:\n",
    "                vals, vc = np.unique(Xc[f], return_counts=True)\n",
    "                # All possible values in training overall for that feature (important for smoothing)\n",
    "                all_vals = np.unique(X_df[f])\n",
    "                prob_dict = {}\n",
    "                total_count = Xc.shape[0]\n",
    "                for v in all_vals:\n",
    "                    # count occurrences of value v in feature f for class cls\n",
    "                    cnt_v = np.sum(Xc[f] == v)\n",
    "                    prob = (cnt_v + self.laplace) / (total_count + self.laplace * len(all_vals))\n",
    "                    prob_dict[v] = prob\n",
    "                self.cat_cond_prob[cls][f] = prob_dict\n",
    "        # numeric parameters (Gaussian)\n",
    "        for cls in self.classes_:\n",
    "            Xc = X_df[y == cls]\n",
    "            self.num_params[cls] = {}\n",
    "            for f in self.num_features:\n",
    "                vals = Xc[f].astype(float).values\n",
    "                mean = np.mean(vals) if len(vals)>0 else 0.0\n",
    "                var  = np.var(vals)  if len(vals)>0 else 1.0\n",
    "                # ensure non-zero variance\n",
    "                if var == 0.0:\n",
    "                    var = 1e-6\n",
    "                self.num_params[cls][f] = (mean, var)\n",
    "        return self\n",
    "    \n",
    "    def _gaussian_logpdf(self, x, mean, var):\n",
    "        # log pdf of normal distribution\n",
    "        return -0.5 * np.log(2*np.pi*var) - ((x-mean)**2) / (2*var)\n",
    "    \n",
    "    def predict(self, X_df):\n",
    "        # X_df: pandas DataFrame, same columns as train\n",
    "        n = X_df.shape[0]\n",
    "        y_pred = np.zeros(n, dtype=int)\n",
    "        for i in range(n):\n",
    "            row = X_df.iloc[i]\n",
    "            class_logpost = {}\n",
    "            for cls in self.classes_:\n",
    "                # log prior\n",
    "                logp = np.log(self.class_priors[cls] + 1e-12)\n",
    "                # categorical features\n",
    "                for f in self.cat_features:\n",
    "                    val = row[f]\n",
    "                    # If unseen value (shouldn't happen because we used overall training set values), handle\n",
    "                    prob = self.cat_cond_prob[cls][f].get(val, None)\n",
    "                    if prob is None:\n",
    "                        # smooth unseen value: assign minimal probability\n",
    "                        all_vals = list(self.cat_cond_prob[cls][f].keys())\n",
    "                        prob = self.laplace / (self.laplace * (len(all_vals)+1))\n",
    "                    logp += np.log(prob + 1e-12)\n",
    "                # numeric features (Gaussian logpdf)\n",
    "                for f in self.num_features:\n",
    "                    mean, var = self.num_params[cls][f]\n",
    "                    logp += self._gaussian_logpdf(row[f], mean, var)\n",
    "                class_logpost[cls] = logp\n",
    "            # choose class with highest log posterior\n",
    "            preds_sorted = sorted(class_logpost.items(), key=lambda x: x[1], reverse=True)\n",
    "            y_pred[i] = preds_sorted[0][0]\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "901a0033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes training complete.\n",
      "Bank dataset (Naive Bayes) evaluation (macro):\n",
      "Accuracy = 0.8404\n",
      "Precision = 0.6317\n",
      "Recall = 0.6400\n",
      "F1-score = 0.6356\n",
      "Confusion matrix (rows=true classes in order 0,1):\n",
      "[[7190  762]\n",
      " [ 681  410]]\n"
     ]
    }
   ],
   "source": [
    "# Cell B5 - Train and evaluate the Hybrid Naive Bayes\n",
    "nb = HybridNaiveBayes(categorical_features=cat_cols, numeric_features=num_cols, laplace=1.0)\n",
    "nb.fit(X_train_bank, y_train_bank)\n",
    "print(\"Naive Bayes training complete.\")\n",
    "\n",
    "y_pred_bank = nb.predict(X_test_bank)\n",
    "\n",
    "acc_b = accuracy(y_test_bank, y_pred_bank)\n",
    "prec_b, rec_b, f1_b = precision_recall_f1(y_test_bank, y_pred_bank, average='macro')\n",
    "cm_b, _ = confusion_matrix(y_test_bank, y_pred_bank)\n",
    "\n",
    "print(\"Bank dataset (Naive Bayes) evaluation (macro):\")\n",
    "print(f\"Accuracy = {acc_b:.4f}\")\n",
    "print(f\"Precision = {prec_b:.4f}\")\n",
    "print(f\"Recall = {rec_b:.4f}\")\n",
    "print(f\"F1-score = {f1_b:.4f}\")\n",
    "print(\"Confusion matrix (rows=true classes in order 0,1):\")\n",
    "print(cm_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17754fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
